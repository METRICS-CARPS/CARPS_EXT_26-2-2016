---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

[PILOT/COPILOT - TEXT IN SQUARE BRACKETS IS HERE FOR GUIDANCE. COPILOT PLEASE DELETE BEFORE KNITTING THE FINAL REPORT]

# Report Details

[PILOT/COPILOT ENTER RELEVANT REPORT DETAILS HERE]

```{r}
articleID <- "EXT_26-2-2016" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- "pilot" # specify whether this is the 'pilot' report or 'final' report
pilotNames <- "Mackenzie Leake" # insert the pilot's name here e.g., "Tom Hardwicke".  If there are multiple cpilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- NA # # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- NA # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- NA # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- NA # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- NA # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- NA # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

[PILOT/COPILOT write a brief summary of the methods underlying the target outcomes written in your own words]

The study began with 1437 particpants on MTurk, but the final sample 
contained 1085 participants, after restricting the sample to those who 
reported they are heterosexual and who passed the attention check in 
the IAT. The authors designed a custom Implicit Association Test (IAT) 
to measure participants' implicit associations between homosexuality 
and sexual promiscuity. They randomly assigned participants to either 
the "gay men" or "lesbian" condition. There were four trials in the IAT. 
For two trials the words "gay" and "promiscuous" were presented on one 
side of the screen, and the words "straight" and "monogamous" were 
presented on the other side of the screen, and for the other two trials 
the words were placed on opposite sides. In each trial participants saw 
five words related to either promiscuity or monogamy and images of gay 
or lesbian couples. The authors compared the response times of 
associations between "gay" and "promiscuous" and "gay" and "monogamous" 
to measure particpants' mental associations between the terms "gay" and 
"promiscuous."

------

#### Target outcomes: 

[PILOT copy and paste the target outcomes identified in targetOutcomes.md]  

The mean IAT score across both conditions was 0.59 (SD =
0.42), which indicates that participants’ reaction times were,
on average, 0.59 standard deviations faster when “gay” and
“promiscuous” were paired than when “gay” and “monogamous”
were paired. Participants in the lesbians condition
showed slightly greater implicit associations between “gay”
and “promiscuous” (M = 0.62, SD = 0.42) than participants
in the gay-men condition did (M = 0.56, SD = 0.41),
t(1083) = 2.56, p < .05. Follow-up analyses revealed that
this effect was qualified by a significant interaction with
gender, F(1, 1081) = 7.42, p < .01. Among men, IAT scores
were significantly higher in the lesbians condition (M =
0.67, SD = 0.39) than in the gay-men condition (M = 0.53,
SD = 0.40), F(1, 1081) = 13.61, p < .001, whereas among
women, there was no significant difference in IAT scores
between the two conditions, F(1, 1081) = 0.01, p = .92.
------

[PILOT/COPILOT DO NOT CHANGE THE CODE IN THE CHUNK BELOW]  

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

[PILOT/COPILOT Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
```

[PILOT/COPILOT DO NOT MAKE CHANGES TO THE CODE CHUNK BELOW]

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
data = read_sav("data/Same-Sex Marriage Dataset.sav")
colnames(data)
```
```{r}
colnames(data)[1] = "id"
```

# Step 3: Tidy data

```{r}
#get ERROR_latency columns
error_latency_cols <- colnames(data)[grep(glob2rx("*_ERROR_latency"), colnames(data))]
#get CORRECT_latency columns
correct_latency_cols <- colnames(data)[grep(glob2rx("*_CORRECT_latency"), colnames(data))]
#get stimulus columns
stimulus_cols <- colnames(data)[grep(glob2rx("*#*_stimulus"), colnames(data))]
#get latency columns
latency_cols <- colnames(data)[grep(glob2rx("*#*_latency"), colnames(data))]

#get the names of all the columns for latency and stimuli
ignore_indices  <- append(grep(glob2rx("*_ERROR_latency"), colnames(data)), grep(glob2rx("*#*_stimulus"), colnames(data)))
ignore_indices <- append(ignore_indices, grep(glob2rx("*#*_latency"), colnames(data)))
ignore_indices <- append(ignore_indices, grep(glob2rx("*_CORRECT_latency"), colnames(data)))
ignore_indices <- unique(ignore_indices)
ignore_names <- colnames(data)[ignore_indices]

#get all the cols that don't have to do with the actual IAT trials
use_indices <- setdiff(seq(1,length(colnames(data))),ignore_indices)
use_names <- colnames(data)[use_indices]

dim(data)

gathered_data <- data %>% 
  gather(subj, value, ignore_names)

dim(gathered_data)

#now we have rows, but we want things grouped by task/response
IAT_nums <- str_extract(error_latency_cols, "[0-9]{4}")
task_nums <- str_extract(error_latency_cols, "Task_[0-9]{1}")
cond_nums <- str_extract(error_latency_cols, "#[0-9]{1}")

spread_data <- gathered_data %>%
  mutate(IAT_num = str_extract(subj, "[0-9]{4}")) %>%
  mutate(task_nums = str_extract(subj, "Task_[0-9]{1}")) %>%
  mutate(task_nums = str_extract(subj, "#[0-9]{1}")) %>%
  mutate(trial = str_extract(subj, "IAT[0-9]{4}_Task_[0-9]{1}#[0-9]{1}")) %>%
  mutate(measurement_type = substr(subj, start=18, stop=length(value)))

dim(spread_data)
measurement_types = unique(spread_data["measurement_type"])

spread_data2 <- spread_data %>%
  mutate(isErrorLatency = measurement_type == "ERROR_latency") %>%
  mutate(isCorrectLatency = measurement_type == "CORRECT_latency") %>%
  mutate(isLatency = measurement_type == "latency") %>%
  mutate(isStimulus = measurement_type == "stimulus")
  
spread_data2[100:110, 115:125]
trial_names = str_extract(ignore_names, "IAT[0-9]{4}_Task_[0-9]{1}#[0-9]{1}")

grouped_data <- spread_data2 %>% 
  group_by(measurement_type)

dim(grouped_data)
```

# Step 4: Run analysis

## Pre-processing

[you can remove this section if no pre-processing is required]

```{r}
```

## Descriptive statistics

```{r}
```

## Inferential statistics

```{r}
```

# Step 5: Conclusion

[Please include a text summary describing your findings. If this reproducibility check was a failure, you should note any suggestions as to what you think the likely cause(s) might be.]
  
[PILOT/COPILOT ENTER RELEVANT INFORMATION BELOW]

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

[PILOT/COPILOT DOD NOT EDIT THE CODE CHUNK BELOW]

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
